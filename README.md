# ğŸ“Š Data Warehouse Pipeline com Airbyte, Airflow e dbt

Este projeto implementa um pipeline ELT utilizando **Airbyte**, **Apache Airflow**, **Docker** e **dbt**. Ele automatiza a extraÃ§Ã£o de dados (ex: Meta Ads), realiza transformaÃ§Ãµes com dbt e gerencia toda a orquestraÃ§Ã£o via Airflow.

---

## ğŸš€ Tecnologias Utilizadas

- [Airbyte](https://airbyte.com/) (instalado separadamente via `abctl`)
- [Apache Airflow](https://airflow.apache.org/)
- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- [dbt (Data Build Tool)](https://www.getdbt.com/)
- Python 3.12

---

## ğŸ“ Estrutura do Projeto

```
data-warehouse-main/
â”œâ”€â”€ dags/                       # DAGs do Airflow
â”‚   â”œâ”€â”€ meta_ads_elt_pipeline.py
â”‚   â””â”€â”€ dbt/
â”‚       â””â”€â”€ dbt_biera/          # Projeto dbt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Como Rodar o Projeto

1. **Clone o repositÃ³rio**:

```bash
git clone <URL_DO_REPOSITORIO>
cd data-warehouse-main
```

2. **Suba o ambiente do Airflow**:

```bash
docker-compose up --build
```

3. **Instale e inicie o Airbyte separadamente (via `abctl`)**:

```bash
brew tap airbytehq/tap
brew install abctl
abctl local install
```

Acesse o Airbyte em `http://localhost:8000` (ou a porta que vocÃª definiu).

4. **Configure as conexÃµes no Airbyte**:

Crie as conexÃµes (por exemplo, Meta Ads â†’ PostgreSQL) e copie o `connectionId` de cada uma.

5. **No Airflow, acesse `Admin > Variables` e adicione**:

- `AIRBYTE_META_ADS_POSTGRES_CONNECTION_ID`
- `AIRBYTE_GS_CONNECTION_ID`
- `AIRBYTE_USERNAME`
- `AIRBYTE_PASSWORD`

---

## ğŸ§  O que o DAG `meta_ads_elt_pipeline` faz?

- Aciona sincronizaÃ§Ã£o no Airbyte
- Monitora status da execuÃ§Ã£o
- Executa transformaÃ§Ãµes com dbt-core
- Orquestra e monitora tudo na interface do Airflow

---

## âœ… Requisitos

- Docker e Docker Compose
- Airbyte instalado separadamente com `abctl`
- ConexÃµes e variÃ¡veis configuradas no Airflow
- Fonte de dados integrada ao Airbyte
- Projeto dbt funcional

### ğŸ› ï¸ Como Adicionar Novos Pipelines

Este projeto integra Airbyte, Airflow e DBT para construÃ§Ã£o de pipelines de dados. Siga as etapas abaixo para adicionar um novo pipeline completo ao ecossistema:

---

#### 1. ğŸ”Œ Criar a ConexÃ£o no Airbyte

- Acesse a interface do **Airbyte**.
- Crie uma nova conexÃ£o entre a origem e o destino desejado.
- ApÃ³s criada, copie o valor do campo `connectionId`.

---

#### 2. âš™ï¸ Criar a VariÃ¡vel no Airflow

- No **Airflow**, acesse o menu **Admin > Variables**.
- Crie uma nova variÃ¡vel com o seguinte formato de nome:

  ```
  AIRBYTE_CONNECTION_ID_<NOME_DO_PIPELINE>
  ```

- O valor da variÃ¡vel deve ser o `connectionId` copiado no passo anterior.
- Esta variÃ¡vel serÃ¡ usada pela DAG para orquestrar a sincronizaÃ§Ã£o dos dados.

---

#### 3. ğŸ“… Criar a DAG no Airflow

- Crie uma nova DAG Python na pasta `dags/`.
- Utilize como modelo uma das DAGs existentes para manter o padrÃ£o do projeto.
- A DAG deve:
  - Buscar o `connectionId` via variÃ¡vel de ambiente.
  - Utilizar o `AirbyteTriggerSyncOperator` para iniciar a sincronizaÃ§Ã£o.
  - Monitorar o status com o `AirbyteJobSensor`.

---

#### 4. ğŸ§ª Desenvolver os Models no DBT

- Crie os arquivos `.sql` correspondentes ao pipeline na pasta `dbt/models/`.
- Siga as boas prÃ¡ticas do DBT:
  - Use `stg_` para modelos de *staging*.
  - Use `fct_` ou `dim_` para modelos finais (*fato* ou *dimensÃ£o*).
- Atualize os arquivos `schema.yml` e `dbt_project.yml` se necessÃ¡rio.

---

#### 5. ğŸ” Integrar ExecuÃ§Ã£o DBT na DAG

- ApÃ³s a sincronizaÃ§Ã£o dos dados com o Airbyte, adicione tasks para executar os modelos DBT.
- Utilize os operadores:
  - `DBTRunOperator`: para executar os modelos.
  - `DBTTestOperator`: para rodar os testes.

- A ordem das tasks deve ser:
  1. `sync_airbyte`
  2. `run_dbt`
  3. `test_dbt`

---

ğŸ“Œ **Dica:**  

> **Nota:** A DAG `meta_ads_elt_pipeline.py` Ã© usada apenas como exemplo.  
> Embora ela esteja configurada para o conector do **Meta Ads**, o projeto nÃ£o Ã© limitado a esse caso de uso.  
> VocÃª pode usar esse exemplo como base para criar pipelines de qualquer origem e destino compatÃ­veis com o Airbyte.  
> 
> As variÃ¡veis `AIRBYTE_META_ADS_POSTGRES_CONNECTION_ID` e `AIRBYTE_GS_CONNECTION_ID` sÃ£o especÃ­ficas do exemplo e **nÃ£o sÃ£o obrigatÃ³rias para o projeto como um todo**.  
> 
> No entanto, **as variÃ¡veis globais** abaixo **devem estar configuradas** no Airflow, pois sÃ£o utilizadas para autenticaÃ§Ã£o com a API do Airbyte:
>
> - `AIRBYTE_USERNAME`
> - `AIRBYTE_PASSWORD`


VocÃª pode usar a DAG `meta_ads_elt_pipeline.py` (em `dags/`) como referÃªncia de estrutura.
